<!doctype html><html lang=en-us><head><title>Paper Trail // a dystopian journey into the heart of tech</title>
<meta charset=utf-8><meta name=generator content="Hugo 0.127.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Pruthvi Kumar"><meta name=description content><link rel=stylesheet href=/blog/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css><link rel=stylesheet href=/blog/css/custom.min.a1c16b97fbe8a2fd96bb78a6a5a8ebb79f1b65243c35465b41c2deefc53f5614.css integrity="sha256-ocFrl/voov2Wu3impajrt58bZSQ8NUZbQcLe78U/VhQ=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>document.addEventListener("DOMContentLoaded",e=>{console.log("DOM fully loaded and parsed"),typeof mermaid!="undefined"?(console.log("Mermaid found, initializing..."),mermaid.initialize({startOnLoad:!0,theme:"dark",themeVariables:{darkMode:!0,background:"#2d2d2d",primaryColor:"#ff9800",secondaryColor:"#2196f3",tertiaryColor:"#4caf50",primaryTextColor:"#fff",secondaryTextColor:"#ddd",lineColor:"#999",textColor:"#fff",mainBkg:"#2d2d2d",nodeBorder:"#7f7f7f",clusterBkg:"#3d3d3d",clusterBorder:"#888",titleColor:"#f8f8f2"}})):console.error("Mermaid not found!")})</script></head><body><header class=app-header><a href=https://1x-eng.github.io/blog/><img class=app-header-avatar src=/blog/avatar.jpg alt="Pruthvi Kumar"></a>
<span class=app-header-title>a dystopian journey into the heart of tech</span><nav class=app-header-menu><a class=app-header-menu-item href=/blog/>Home</a></nav><p>confessions of a serial key puncher: the life and times of a coder.</p></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>Paper Trail</h1><div class=post-meta><div><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
Feb 13, 2026</div><div><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
5 min read</div></div></header><div class=post-content><h2 id=trace-first-instrumentation-a-mental-model>trace first instrumentation: a mental model</h2><p>over the years i&rsquo;ve found myself in variations of the same conversation: explaining trace-first instrumentation to a room that&rsquo;s skeptical. not hostile, just unconvinced. the counterarguments are always reasonable. &ldquo;logs are essential for debugging.&rdquo; &ldquo;we need structured logs for incident investigation.&rdquo; &ldquo;you can&rsquo;t replace logs with traces.&rdquo;</p><p>and they&rsquo;re right. you can&rsquo;t replace logs with traces. that&rsquo;s not the argument.</p><h2 id=the-argument>the argument</h2><p>the canonical framing from google SRE and cindy sridharan&rsquo;s <em>distributed systems observability</em>: &ldquo;metrics for detection, traces for location, logs for explanation.&rdquo; three pillars, distinct purposes. i&rsquo;m not contesting that.</p><p>what i am contesting is the implementation order.</p><p>the default approach i&rsquo;ve seen at multiple organisations:</p><ol><li>add logging calls at key points in the codebase</li><li>when debugging, search logs for errors</li><li>extract correlation IDs, grep across services</li><li>reconstruct the request path from timestamps</li><li>later, add tracing as a separate instrumentation layer</li><li>maintain both</li></ol><p>the trace-first approach inverts the order:</p><ol><li>every unit of work is a span</li><li>context attaches as span attributes</li><li>the fmt layer emits logs as a byproduct</li><li>the same <code>tracing::info!()</code> call writes to spans and stdout</li></ol><p>same three pillars. different order of operations. different debugging experience.</p><h2 id=some-context>some context</h2><p>i picked this up from a staff engineer at a previous company. his framing stuck with me:</p><p>traces are the unit of work in a distributed system. logs and metrics are projections of that unit, not peers to it.</p><p>when you instrument trace-first, you&rsquo;re declaring &ldquo;this function is a unit of work.&rdquo; the span carries context automatically. child spans inherit it. across service boundaries, if you propagate trace context, the request lifecycle is one queryable tree.</p><p>it reframed how i think about instrumentation. i don&rsquo;t start with &ldquo;what should i log?&rdquo; anymore. i start with &ldquo;what&rsquo;s the unit of work?&rdquo;</p><p>when you instrument logs-first, each log line is independent. correlating them requires discipline, convention, and consistent implementation across teams. miss one correlation ID? the chain breaks. (to be fair, miss one trace context propagation and you get orphan spans. the failure mode exists in both worlds, but traces make the gap visible in the UI rather than silently missing.)</p><p>i&rsquo;ve worked at a few places where suggesting this pattern was met with surprise. not resistance - more like &ldquo;that&rsquo;s not how we&rsquo;ve done it.&rdquo; which is fair. logs are familiar. the tooling ecosystem grew up around logs.</p><p>but after using trace-first in production, i stopped reconstructing request paths from timestamps. that alone was worth the switch.</p><h2 id=a-working-example>a working example</h2><p>i put together a minimal demo to make this concrete: <a href=https://github.com/1x-eng/paper-trail>github.com/1x-eng/paper-trail</a></p><p>two rust services. gateway receives http requests, forwards to worker. worker processes, fails ~10% of the time, hits a slow path ~5% of the time. both export traces to jaeger (or any OTEL-compliant backend) via opentelemetry.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker compose up --build
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>1..20<span class=o>}</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>  curl -s -X POST http://localhost:3000/process <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -d <span class=s2>&#34;{\&#34;id\&#34;: \&#34;req-</span><span class=nv>$i</span><span class=s2>\&#34;, \&#34;payload\&#34;: \&#34;test data </span><span class=nv>$i</span><span class=s2>\&#34;}&#34;</span>
</span></span><span class=line><span class=cl>  <span class=nb>echo</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span></code></pre></td></tr></table></div></div><p>open http://localhost:16686.</p><p><img src=/blog/paper_trail_jaeger_home.png alt=jaeger-search-view>
<em>20 traces. red dots are errors. the tall green dot is a slow request (~696ms vs ~100ms normal). the symptoms are visible before clicking anything.</em></p><p>click a failed trace. expand it.</p><p><img src=/blog/paper_trail_spans_expanded.png alt=jaeger-trace-expanded>
<em>span tree across both services: gateway http_request → validate_input → dispatch_to_worker → worker process_payload → simulate_work. the error is on simulate_work.</em></p><p>the span detail panel shows:</p><ul><li><code>otel.status_code = ERROR</code></li><li><code>otel.status_message = rate limited: upstream returned 429</code></li><li><code>work.success = false</code></li></ul><p>below that, the <strong>Logs</strong> section contains the span events. the <code>tracing::info!()</code> and <code>tracing::error!()</code> calls, attached to this span, with context. <code>exception.message</code>, <code>sleep_ms</code>, all queryable.</p><p>for latency, the flamegraph view:</p><p><img src=/blog/paper_trail_flamegraph.png alt=jaeger-flamegraph>
<em>the slow path is visible without digging.</em></p><h2 id=but-you-still-need-logs>but you still need logs</h2><p>yes. and they&rsquo;re there.</p><p>every <code>tracing::info!()</code> goes to two places:</p><ol><li>the otel layer → span events in traces</li><li>the fmt layer → stdout</li></ol><p>run <code>docker logs worker</code>:</p><p><img src=/blog/paper_trail_docker_logs.png alt=docker-logs>
<em>stdout logs with structured context. payload.id, work_type, exception.message. the same <code>tracing::error!()</code> call produced both the span event and this line.</em></p><p>the argument isn&rsquo;t &ldquo;don&rsquo;t have logs.&rdquo; it&rsquo;s &ldquo;don&rsquo;t build a separate logging strategy.&rdquo; the fmt layer gives you stdout from the same instrumentation.</p><h2 id=the-debugging-workflow>the debugging workflow</h2><h3 id=logs-first>logs-first</h3><ol><li>error reported</li><li>search logs for the error message</li><li>find it in service B</li><li>extract the request id</li><li>search service A logs for that id</li><li>check timestamps to reconstruct order</li><li>repeat for each service in the path</li></ol><h3 id=trace-first>trace-first</h3><ol><li>error reported</li><li>open tracing ui, filter by error</li><li>click the trace</li><li>see the span tree</li><li>click the red span</li><li>read the attributes</li></ol><p>the difference in time-to-root-cause depends on system complexity. for a two-service demo, it&rsquo;s marginal. for ten services with async communication, it&rsquo;s significant.</p><h2 id=runtime-errors-panics-oom>runtime errors, panics, OOM</h2><p>if the process dies mid-trace, the otel batch exporter might not flush.</p><p>true. but a <code>log::info!()</code> with buffered output has the same problem. the difference is traces add one more buffering layer (the batch exporter), so the window of data loss is wider. that said, this is an infrastructure concern. health checks, restart counts, container events. not an application instrumentation decision.</p><p>for crashes:</p><ul><li>the fmt layer already wrote to stdout before the crash</li><li>docker/k8s captures stdout</li><li>infrastructure monitoring tells you about restarts regardless of application instrumentation</li></ul><p>application observability and infrastructure observability are different layers.</p><h2 id=economics>economics</h2><p>trace-first instrumentation supports tail sampling. emit from the application, make sampling decisions at the collector based on outcome. error? keep. slow? keep. normal success? sample down.</p><p>log-first instrumentation typically uses head sampling or no sampling. the decision happens at emit time, before you know the outcome.</p><p>the arithmetic:</p><ul><li>head sampling at 5%, error rate 0.1% → 0.005% probability of capturing a failing request</li><li>tail sampling at 100%, keep all errors, sample successes to 10% → 100% error capture, ~10% storage</li></ul><p>tail sampling requires trace-first instrumentation. head sampling is what you&rsquo;re left with when logs are independent.</p><h2 id=caveats>caveats</h2><p>this is not universal advice.</p><ul><li>if you have years of investment in structured logging pipelines, ripping that out is expensive and risky</li><li>if your team hasn&rsquo;t used jaeger/dd/tempo/honeycomb, there&rsquo;s a learning curve</li><li>some orgs can&rsquo;t justify the collector infrastructure cost</li></ul><p>what i am saying: if you&rsquo;re starting fresh, or reconsidering your approach, trace-first is worth evaluating. the debugging workflow is different.</p><p>skeptical? run <a href=https://github.com/1x-eng/paper-trail>paper-trail</a>, break it, see what the experience looks like.</p><hr><p><em>if you run it and find something off, open an issue.</em></p></div><div class=post-footer></div></article></main></body></html>